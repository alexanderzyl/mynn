# [Pay Attention to MLPs (gMLP)](https://nn.labml.ai/transformers/gmlp/index.html)

This is a [PyTorch](https://pytorch.org) implementation of the paper
[Pay Attention to MLPs](https://papers.labml.ai/paper/2105.08050).

This paper introduces a Multilayer Perceptron (MLP) based architecture with gating,
which they name **gMLP**. It consists of a stack of $L$ *gMLP* blocks.

Here is [the training code](https://nn.labml.ai/transformers/gmlp/experiment.html) for a gMLP model based autoregressive model.

[![View Run](https://img.shields.io/badge/labml-experiment-brightgreen)](https://app.labml.ai/run/01bd941ac74c11eb890c1d9196651a4a)
